{"cells":[{"metadata":{},"cell_type":"markdown","source":"# More Advanced Kaggle Starter for House of Blocks Kaggle In-Class Competition\n\nThis workbook is a starter code for the [Kaggle In-Class House of Blocks Competition](https://www.kaggle.com/c/applications-of-deep-learning-wustl-fall-2020)  This competition is one of the assignments for [T81-558: Applications of Deep Neural Netw1orks](https://sites.wustl.edu/jeffheaton/t81-558/) at [Washington University in St. Louis](https://www.wustl.edu).\n\nThis notebook gets a better score than my previous starter, by using transfer learning.  The notebook also implements basic feature importance. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nPATH = \"/kaggle/input/applications-of-deep-learning-wustl-fall-2020/final-kaggle-data/\"\nPATH_TRAIN = os.path.join(PATH, \"train.csv\")\nPATH_TEST = os.path.join(PATH, \"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we check versions and if the GPU is available. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# What version of Python do you have?\nimport sys\n\nimport tensorflow.keras\nimport pandas as pd\nimport sklearn as sk\nimport tensorflow as tf\n\nprint(f\"Tensor Flow Version: {tf.__version__}\")\nprint(f\"Keras Version: {tensorflow.keras.__version__}\")\nprint()\nprint(f\"Python {sys.version}\")\nprint(f\"Pandas {pd.__version__}\")\nprint(f\"Scikit-Learn {sk.__version__}\")\nprint(\"GPU is\", \"available\" if tf.test.is_gpu_available() \\\n      else \"NOT AVAILABLE\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we prepare to read the training data (that we have labels for) and the test data that we must predict and send to Kaggle."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(PATH_TRAIN)\ndf_test = pd.read_csv(PATH_TEST)\n\ndf_train = df_train[df_train.id != 1300]\n\ndf_train['filename'] = df_train[\"id\"].astype(str)+\".png\"\ndf_train['stable'] = df_train['stable'].astype(str)\n\ndf_test['filename'] = df_test[\"id\"].astype(str)+\".png\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perform a basic balance plot.  This data is fairly well balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndf_train.stable.value_counts().plot(kind='bar')\nplt.title('Labels counts')\nplt.xlabel('Stable')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to use early stopping.  To do this, we need a validation set.  We will break the data into 80 percent test data and 20 validation.  Do not confuse this validation data with the test set provided by Kaggle.  This validation set is unique to your program and is just used for early stopping."},{"metadata":{"trusted":true},"cell_type":"code","source":"# subset\n# df_train_sub = df_train.head(4098)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add subset!!!!\nTRAIN_PCT = 0.9\nTRAIN_CUT = int(len(df_train) * TRAIN_PCT)\n\ndf_train_cut = df_train[0:TRAIN_CUT]\ndf_validate_cut = df_train[TRAIN_CUT:]\n\nprint(f\"Training size: {len(df_train_cut)}\")\nprint(f\"Validate size: {len(df_validate_cut)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we create the generators that will provide the images to the neural network as it is trained.  We normalize the images so that the RGB colors between 0-255 become ratios between 0 and 1.  We also use the **flow_from_dataframe** generator to connect the Pandas dataframe to the actual image files. We see here a straightforward implementation; you might also wish to use some of the image transformations provided by the data generator.\n\nThe **HEIGHT** and **WIDTH** constants specify the dimensions that the image will be scaled (or expanded) to. It is probably not a good idea to expand the images."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\n\nWIDTH = 224\nHEIGHT = 224\n\ntraining_datagen = ImageDataGenerator(\n  rescale = 1./255,\n  horizontal_flip=True,\n  vertical_flip=True,\n  fill_mode='nearest')\n\ntrain_generator = training_datagen.flow_from_dataframe(\n        dataframe=df_train_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"stable\",\n        target_size=(HEIGHT, WIDTH),\n        batch_size=32,\n        shuffle=True,\n        class_mode='binary')\n#         class_mode='binary'\n\nvalidation_datagen = ImageDataGenerator(rescale = 1./255)\n\nval_generator = validation_datagen.flow_from_dataframe(\n        dataframe=df_validate_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"stable\",\n        target_size=(HEIGHT, WIDTH),\n        batch_size=32,\n        class_mode='binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now create the neural network and fit it.  Some essential concepts are going on here.\n\n* **Batch Size** - The number of training samples that should be evaluated per training step.  Smaller batch sizes, or mini-batches, are generally preferred.\n* **Step** - A training step is one complete run over the batch.  At the end of a step, the weights are updated, and the neural network learns.\n* **Epoch** - An arbitrary point at which to measure results or checkpoint the model.  Generally, an epoch is one complete pass over the training set.  However, when generators are used, the training set size is theoretically infinite. Because of this, we set a **steps_per_epoch** parameter.\n* **validation steps** - The validation set may also be infinite; because of this, we must specify how many steps we wish to validate at the end of each Epoch."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n# from tensorflow.keras.applications import MobileNetV2\n# from tensorflow.keras.applications.mobilenet import preprocess_input\nfrom tensorflow.keras.applications import Xception\n# from tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Activation,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\n\nbase_model = Xception(include_top=False, input_shape=[224,224,3])\n# base_model = VGG19(include_top=False, input_shape=[224,224,3])\nbase_model.trainable = True\nx=base_model.output\n# x=Dense(512,activation='relu')(x)\n# x=BatchNormalization()(x)\n# x=Dropout(0.5)(x)\n# x=Dense(256,activation='relu')(x)\n# x=BatchNormalization()(x)\nx=GlobalAveragePooling2D()(x)\npreds=Dense(1,activation=\"sigmoid\")(x)\n# preds=Dense(2,activation='softmax')(x)\nmodel=Model(inputs=base_model.input,outputs=preds)\nmodel.summary()\n\n# base_model=MobileNet(weights='imagenet',include_top=False,alpha = 1.0,dropout = 0.3,) \n\n# x=base_model.output\n# x=GlobalAveragePooling2D()(x)\n# x=Dense(1024,activation='relu')(x) \n# x=Dense(1024,activation='relu')(x) \n# preds=Dense(2,activation='softmax')(x)\n\n# model=Model(inputs=base_model.input,outputs=preds)\n\n# for layer in model.layers[:20]:\n#     layer.trainable=True\n# for layer in model.layers[20:]:\n#     layer.trainable=True\n\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import keras.backend as K\n# from keras.callbacks import Callback\n\n\n# class LRFinder(Callback):\n    \n#     '''\n#     A simple callback for finding the optimal learning rate range for your model + dataset. \n    \n#     # Usage\n#         ```python\n#             lr_finder = LRFinder(min_lr=1e-5, \n#                                  max_lr=1e-2, \n#                                  steps_per_epoch=np.ceil(epoch_size/batch_size), \n#                                  epochs=3)\n#             model.fit(X_train, Y_train, callbacks=[lr_finder])\n            \n#             lr_finder.plot_loss()\n#         ```\n    \n#     # Arguments\n#         min_lr: The lower bound of the learning rate range for the experiment.\n#         max_lr: The upper bound of the learning rate range for the experiment.\n#         steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n#         epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n        \n#     # References\n#         Blog post: jeremyjordan.me/nn-learning-rate\n#         Original paper: https://arxiv.org/abs/1506.01186\n#     '''\n    \n#     def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n#         super().__init__()\n        \n#         self.min_lr = min_lr\n#         self.max_lr = max_lr\n#         self.total_iterations = steps_per_epoch * epochs\n#         self.iteration = 0\n#         self.history = {}\n        \n#     def clr(self):\n#         '''Calculate the learning rate.'''\n#         x = self.iteration / self.total_iterations \n#         return self.min_lr + (self.max_lr-self.min_lr) * x\n        \n#     def on_train_begin(self, logs=None):\n#         '''Initialize the learning rate to the minimum value at the start of training.'''\n#         logs = logs or {}\n#         K.set_value(self.model.optimizer.lr, self.min_lr)\n        \n#     def on_batch_end(self, epoch, logs=None):\n#         '''Record previous batch statistics and update the learning rate.'''\n#         logs = logs or {}\n#         self.iteration += 1\n\n#         self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n#         self.history.setdefault('iterations', []).append(self.iteration)\n\n#         for k, v in logs.items():\n#             self.history.setdefault(k, []).append(v)\n            \n#         K.set_value(self.model.optimizer.lr, self.clr())\n \n#     def plot_lr(self):\n#         '''Helper function to quickly inspect the learning rate schedule.'''\n#         plt.plot(self.history['iterations'], self.history['lr'])\n#         plt.yscale('log')\n#         plt.xlabel('Iteration')\n#         plt.ylabel('Learning rate')\n#         plt.show()\n        \n#     def plot_loss(self):\n#         '''Helper function to quickly observe the learning rate experiment results.'''\n#         plt.plot(self.history['lr'], self.history['loss'])\n#         plt.xscale('log')\n#         plt.xlabel('Learning rate')\n#         plt.ylabel('Loss')\n#         plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras.callbacks import LearningRateScheduler\n\ndef step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n    '''\n    Wrapper function to create a LearningRateScheduler with step decay schedule.\n    '''\n    def schedule(epoch):\n        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n    \n    return LearningRateScheduler(schedule)\n\nlr_sched = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)\n\n# model.fit(X_train, Y_train, callbacks=[lr_sched])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom keras.callbacks import Callback\nimport keras.backend as K\nimport numpy as np\n\nclass SGDRScheduler(Callback):\n    '''Cosine annealing learning rate scheduler with periodic restarts.\n    # Usage\n        ```python\n            schedule = SGDRScheduler(min_lr=1e-5,\n                                     max_lr=1e-2,\n                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n                                     lr_decay=0.9,\n                                     cycle_length=5,\n                                     mult_factor=1.5)\n            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n        ```\n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n        lr_decay: Reduce the max_lr after the completion of each cycle.\n                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n        cycle_length: Initial number of epochs in a cycle.\n        mult_factor: Scale epochs_to_restart after each full cycle completion.\n    # References\n        Blog post: jeremyjordan.me/nn-learning-rate\n        Original paper: http://arxiv.org/abs/1608.03983\n    '''\n    def __init__(self,\n                 min_lr,\n                 max_lr,\n                 steps_per_epoch,\n                 lr_decay=1,\n                 cycle_length=10,\n                 mult_factor=2):\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.lr_decay = lr_decay\n\n        self.batch_since_restart = 0\n        self.next_restart = cycle_length\n\n        self.steps_per_epoch = steps_per_epoch\n\n        self.cycle_length = cycle_length\n        self.mult_factor = mult_factor\n\n        self.history = {}\n\n    def clr(self):\n        '''Calculate the learning rate.'''\n        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n        \n        return lr\n\n    def on_train_begin(self, logs={}):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.max_lr)\n\n    def on_batch_end(self, batch, logs={}):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        self.batch_since_restart += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_epoch_end(self, epoch, logs={}):\n        '''Check for end of current cycle, apply restarts when necessary.'''\n        if epoch + 1 == self.next_restart:\n            self.batch_since_restart = 0\n            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n            self.next_restart += self.cycle_length\n            self.max_lr *= self.lr_decay\n            self.best_weights = self.model.get_weights()\n\n    def on_train_end(self, logs={}):\n        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n        self.model.set_weights(self.best_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"schedule = SGDRScheduler(min_lr=1e-5,\n                         max_lr=1e-4,\n                         steps_per_epoch=np.ceil(1036/32),\n                         lr_decay=0.9,\n                         cycle_length=5,\n                         mult_factor=1.5)\n# model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nvalidation_steps = len(df_validate_cut)\n# opt = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics=['accuracy'])\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='auto',\n        restore_best_weights=True)\nhistory = model.fit(train_generator, epochs=25, steps_per_epoch=1036, \n                    validation_data = val_generator, \n                    verbose = 1, validation_steps=20,callbacks=[monitor,schedule])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance\n\nWe will use the [Grad-CAM algorithm](https://keras.io/examples/vision/grad_cam/) to determine feature importance for a few images."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Display\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\ndef get_img_array(img_path, size):\n    # `img` is a PIL image of size 299x299\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(\n    img_array, model, last_conv_layer_name, classifier_layer_names\n):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n    return heatmap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Activation for last CONV layer.\nlast_conv_layer_name = \"block14_sepconv2_act\"\n\n\nclassifier_layer_names = [\n    \"global_average_pooling2d\",\n    \"dense\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_grad_cam(filename):\n    img_path = os.path.join(PATH, filename)\n    img_array = get_img_array(img_path, size=(HEIGHT, WIDTH))\n    img_array /= 255.0\n    preds = model.predict(img_array)\n    print(f\"Prediction: {preds}\")\n    #img = Image(img_path)\n    #display(img)\n    heatmap = make_gradcam_heatmap(\n        img_array, model, last_conv_layer_name, classifier_layer_names\n    )\n    plt.matshow(heatmap)\n    plt.show()\n\n    # We load the original image\n    img = keras.preprocessing.image.load_img(img_path)\n    img = keras.preprocessing.image.img_to_array(img)\n\n    # We rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * 0.4 + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\n#     Display Grad CAM\n    display(superimposed_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unstable\nprocess_grad_cam(\"8.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stable\nprocess_grad_cam(\"1.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Submission\n\nNow that the neural network is trained; we need to generate a submit CSV file to send to Kaggle.  We will use nearly the same technique to build the submit file.  However, these essential points that we must address:\n\n* We do not want the data generator to create an infinite date like we did when training.  We have a fixed number of cases to score for the Kaggle submit; we only want to process them.\n* We do not want the data generator to randomize the samples' order like it did when training. Therefore we set shuffle to false.\n* We want to always start at the beginning of the data, so we reset the generator.\n\nThese ensure that the predictions align with the id's."},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_datagen = ImageDataGenerator(rescale = 1./255)\n\nsubmit_generator = submit_datagen.flow_from_dataframe(\n        dataframe=df_test,\n        directory=PATH,\n        x_col=\"filename\",\n        batch_size = 1,\n        shuffle = False,\n        target_size=(HEIGHT, WIDTH),\n        class_mode=None)\nsubmit_generator.reset()\npred = model.predict(submit_generator,steps=len(df_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modeified !!!\ndf_submit = pd.DataFrame({\"id\":df_test['id'],'stable':pred.flatten()})\ndf_submit.to_csv(\"/kaggle/working/submit_step_lr.csv\",index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}